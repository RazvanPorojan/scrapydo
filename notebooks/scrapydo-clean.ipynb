{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapydo-clean.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RazvanPorojan/scrapydo/blob/master/notebooks/scrapydo-clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Xsz2iouc0I7f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ScrapyDo Overview"
      ]
    },
    {
      "metadata": {
        "id": "AcqDOGpn0I7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[ScrapyDo](https://github.com/darkrho/scrapydo) is a [crochet](https://github.com/itamarst/crochet)-based blocking API for [Scrapy](http://scrapy.org). It allows the usage of Scrapy as a library, mainly aimed to be used in spiders prototyping and data exploration in [IPython notebooks](http://ipython.org/notebook.html)."
      ]
    },
    {
      "metadata": {
        "id": "8tNpTuYi0I7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook we are going to show how to use `scrapydo` and how it helps to rapidly crawl and explore data. Our main premise is that we want to crawl the internet as a mean to analysis data and not as an end."
      ]
    },
    {
      "metadata": {
        "id": "XA2CSJU90I7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "QiHR46h00I7s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function `setup` must be called before any call to other functions."
      ]
    },
    {
      "metadata": {
        "id": "ASw4VPsx0Qge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install scrapydo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Isg4UTCJ0I7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scrapydo\n",
        "scrapydo.setup()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnv5mKU30I73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The `fetch` function and highlight helper"
      ]
    },
    {
      "metadata": {
        "id": "ogf7iC400I75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `fetch` function returns a `scrapy.Response` object for a given URL."
      ]
    },
    {
      "metadata": {
        "id": "ghhZRKoF0I77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "response = scrapydo.fetch(\"https://www.linkedin.com/in/cairo-cavalcante-53064738/\")\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(response.text))\n",
        "#print(response.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivxdkTp45_df",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from loginform import fill_login_form\n",
        "\n",
        "class MySpiderWithLogin(scrapy.Spider):\n",
        "    name = 'my-spider'\n",
        "\n",
        "    start_urls = [\n",
        "        'https://www.linkedin.com/in/cairo-cavalcante-53064738/',\n",
        "        'https://www.linkedin.com/in/marius-istrate-b8ab0566/',\n",
        "    ]\n",
        "\n",
        "    login_url = 'https://www.linkedin.com/uas/login'\n",
        "\n",
        "    login_user = 'your-username'\n",
        "    login_password = 'secret-password-here'\n",
        "\n",
        "    def start_requests(self):\n",
        "        # let's start by sending a first request to login page\n",
        "        yield scrapy.Request(self.login_url, self.parse_login)\n",
        "\n",
        "    def parse_login(self, response):\n",
        "        # got the login page, let's fill the login form...\n",
        "        data, url, method = fill_login_form(response.url, response.body,\n",
        "                                            self.login_user, self.login_password)\n",
        "\n",
        "        # ... and send a request with our login data\n",
        "        return scrapy.FormRequest(url, formdata=dict(data),\n",
        "                           method=method, callback=self.start_crawl)\n",
        "\n",
        "    def start_crawl(self, response):\n",
        "        # OK, we're in, let's start crawling the protected pages\n",
        "        for url in self.start_urls:\n",
        "            yield scrapy.Request(url)\n",
        "\n",
        "    def parse(self, response):\n",
        "        # do stuff with the logged in response"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
