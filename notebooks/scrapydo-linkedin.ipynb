{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapydo-overview.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RazvanPorojan/scrapydo/blob/master/notebooks/scrapydo-linkedin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Xsz2iouc0I7f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ScrapyDo Overview"
      ]
    },
    {
      "metadata": {
        "id": "AcqDOGpn0I7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[ScrapyDo](https://github.com/darkrho/scrapydo) is a [crochet](https://github.com/itamarst/crochet)-based blocking API for [Scrapy](http://scrapy.org). It allows the usage of Scrapy as a library, mainly aimed to be used in spiders prototyping and data exploration in [IPython notebooks](http://ipython.org/notebook.html)."
      ]
    },
    {
      "metadata": {
        "id": "8tNpTuYi0I7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook we are going to show how to use `scrapydo` and how it helps to rapidly crawl and explore data. Our main premise is that we want to crawl the internet as a mean to analysis data and not as an end."
      ]
    },
    {
      "metadata": {
        "id": "XA2CSJU90I7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "QiHR46h00I7s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function `setup` must be called before any call to other functions."
      ]
    },
    {
      "metadata": {
        "id": "ASw4VPsx0Qge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install scrapydo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Isg4UTCJ0I7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scrapydo\n",
        "scrapydo.setup()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnv5mKU30I73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The `fetch` function and highlight helper"
      ]
    },
    {
      "metadata": {
        "id": "ogf7iC400I75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `fetch` function returns a `scrapy.Response` object for a given URL."
      ]
    },
    {
      "metadata": {
        "id": "ghhZRKoF0I77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "response = scrapydo.fetch(\"https://www.linkedin.com/in/cairo-cavalcante-53064738/\")\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(response.text))\n",
        "#print(response.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivxdkTp45_df",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from loginform import fill_login_form\n",
        "\n",
        "class MySpiderWithLogin(scrapy.Spider):\n",
        "    name = 'linkedin'\n",
        "\n",
        "    start_urls = [\n",
        "        'https://www.linkedin.com/in/cairo-cavalcante-53064738/',\n",
        "        'https://www.linkedin.com/in/marius-istrate-b8ab0566/',\n",
        "    ]\n",
        "\n",
        "    login_url = 'https://www.linkedin.com/uas/login'\n",
        "\n",
        "    login_user = 'aaaa'\n",
        "    login_password = 'bbbb'\n",
        "\n",
        "    def start_requests(self):\n",
        "        # let's start by sending a first request to login page\n",
        "        yield scrapy.Request(self.login_url, self.parse_login)\n",
        "\n",
        "    def parse_login(self, response):\n",
        "        # got the login page, let's fill the login form...\n",
        "        print(\"login\")\n",
        "        data, url, method = fill_login_form(response.url, response.body,\n",
        "                                            self.login_user, self.login_password)\n",
        "\n",
        "        # ... and send a request with our login data\n",
        "        return scrapy.FormRequest(url, formdata=dict(data),\n",
        "                           method=method, callback=self.start_crawl)\n",
        "\n",
        "    def start_crawl(self, response):\n",
        "        # OK, we're in, let's start crawling the protected pages\n",
        "        print(\"Start Crawl\")\n",
        "        for url in self.start_urls:\n",
        "            yield scrapy.Request(url)\n",
        "\n",
        "    def parse(self, response):\n",
        "        print(\"Parse\")\n",
        "        # do stuff with the logged in response\n",
        "        print(response.body)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rf_RiDwVABtT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1b2ea165-2ade-4071-aa89-e829f279e6cf"
      },
      "cell_type": "code",
      "source": [
        "items = scrapydo.run_spider( MySpiderWithLogin)\n",
        "items"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "login\n",
            "Start Crawl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "JJC50DFVCpD7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.root.setLevel(logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bu0AIdL0_NFf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e681f8b3-179a-4f1e-9847-66297029b71b"
      },
      "cell_type": "code",
      "source": [
        "!pip install loginform"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting loginform\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/d3/267b4304a72b8471250ae550423d5cd2ce9961de76ac3502f5c680da8e6c/loginform-1.2.0.tar.gz\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from loginform) (4.2.6)\n",
            "Building wheels for collected packages: loginform\n",
            "  Building wheel for loginform (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/10/89/8b/7683cae3c30723c102f761bd75befa432999cdb51b00eeffe9\n",
            "Successfully built loginform\n",
            "Installing collected packages: loginform\n",
            "Successfully installed loginform-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}